#!/bin/ksh -p
#
# In order to process the MB Zip files we need to download them.  Now
# as MB finds bugs and fixes them then he edits the old post.  This can
# make it hard to find the latest version.
#
# This script is a nasty screen scraper.  It tracks the two main threads
# MB uses to post to, and downloads them in total.
# Then it goes through the resulting HTML to find the links to the ZIP files
# and downloads those.
#
# The resulting ZIP files are in the ZIPS directory
# The HTML is in the PAGES directory; we keep this just in case the
#    scraping fails, so we can look at it for diagnostics.  But we
#    never use it once the scraping is completed.

rm -rf ZIPS.old PAGES
mv ZIPS ZIPS.old
mkdir ZIPS PAGES || exit
cd PAGES || exit

FORUM=https://www.stardot.org.uk/forums
BASE_URLS="$FORUM/viewtopic.php?f=32&t=8270 $FORUM/viewtopic.php?f=32&t=9049"
DOWNLOAD="$FORUM/download/file.php?id="
PATTERNS="Disc AltD Orig"

typeset -Z3 page=1

for BASE_URL in $BASE_URLS
do
  echo Using $BASE_URL

  url=$BASE_URL

  while [ "$url" != "" ]
  do
    echo
    echo
    echo Getting Page $page
    wget -q -O page$page $url
    ls -l page$page
    echo Searching for next page
    next=$(sed -n 's/.*viewtopic.*start=\([0-9]*\)".*Next.*/\1/p' page$page | uniq)
  
    if [ -n "$next" ]
    then
      echo Starting at $next
      url="$BASE_URL&start=$next"
    else
      url=""
    fi
    let page=page+1
  done
done

echo No more pages
echo

cd ../ZIPS || exit

for pattern in $PATTERNS
do
  echo Searching for zip files for $pattern

  cat ../PAGES/page* | sed -n 's!.*download/file.php?id=\([0-9]*\).*\('$pattern'[0-9].*zip\)<.*!\1 \2!p' | while read id name
  do
    wget -q -O"$name" $DOWNLOAD$id
    ls -l "$name"
  done
done

